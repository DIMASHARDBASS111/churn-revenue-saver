# Telecom Customer Churn Analysis 
# Полный пайплайн анализа от сырых данных до загрузки в PostgreSQL

## Описание проекта

Этот проект представляет собой полный цикл обработки и анализа данных о клиентском оттоке (**churn**) крупного телеком-провайдера.
Цель - воспроизвести реальный производственный пайплайн: от сбора и очистки данных до аналитики, визуализаций, создания дополнительных признаков и сохранения результатов в базу данных.

Работа разделена на четыре логических ноутбука, имитирующих этапы типичного рабочего процесса.

## Структура проекта

```
project/
│── dashboards/
│   ├──Telecom Churn.pbix
│
│── data/
│   ├── raw/         # исходные сырые данные
│   ├── processed/   # очищенные и преобразованные данные
│
│── notebooks/
│   ├── 01_EDA.ipynb
│   ├── 02_data_cleaning.ipynb
│   ├── 03_visuals.ipynb
│   ├── 04_features_SQL.ipynb
│   ├── figures/    # все полученные графики здесь
│
│ 
│── sql/
│   ├── SQL_churn_data.sql
│
│── README.md
│── requirements.txt
│── .env
│── .gitignore
```

### 1. Data Overview (Первичный обзор данных)

В первом ноутбуке проведено:

- загрузка сырых данных

- базовое EDA:

    - распределения числовых признаков

    - анализ категориальных переменных

    - heatmap корреляций

- определение проблем, которые нужно исправить на следующих этапах

Этот ноутбук формирует общее представление о структуре и качестве данных и определяет направления дальнейшей очистки.

### 2. Data Cleaning (Очистка данных)

Второй ноутбук посвящён полноценной очистке:

- обработка пропусков

- исправление проблемных полей (`TotalCharges`, неверный тип данных)

- удаление/исправление аномалий

- стандартизация категорий

Итогом является сохранённый набор чистых данных:
```
data/processed/clean_churn.csv
```

### 3. Visual Analysis (Расширенный визуальный анализ)

В третьем ноутбуке выполняется углублённый визуальный анализ на очищённом датасете. Этот ноутбук служит для поиска и иллюстрации инсайтов, которые затем лягут в отчёт и дашборд.

Что сделано:

- Сравнение распределения `Churn` (проценты и абсолютные значения).

- Гистограмма `tenure` для ушедших клиентов (показывает, что большая часть оттока - в первые месяцы).

- Boxplots для `MonthlyCharges` и `TotalCharges` среди ушедших клиентов (закономерности и выбросы).

- Countplots для ключевых категориальных признаков с hue=`Churn` (`OnlineSecurity`, `Contract`, `PaymentMethod`, `TechSupport`, `InternetService`) - выявление категорий с повышенным риском оттока.

- Таблица процентных значений (churn rate) для `Contract`, `OnlineSecurity`, `PaymentMethod`, `TechSupport`, `InternetService`.

- heatmap между числовыми признаками (включая `TotalCharges` - на этом этапе данные уже очищены).

### 4. Создание Признаков и Экспорт в SQL

Четвёртый ноутбук отвечает за формирование финальных признаков и подготовку данных для BI/SQL.

Созданы дополнительные признаки:

- `contract_length` (1, 12, 24)

- `has_security` (0/1)

- `has_techsupport` (0/1)

- `services_count` (количество дополнительных сервисов)

- `churn_flag` (0/1)

- Выполнена проверка типов и целостности данных.

- Подготовлен финальный файл: `data/processed/churn_final.csv`

Интеграция с PostgreSQL:

- конфигурация загружается из .env

- создаётся SQLAlchemy engine,

- таблица churn_data записывается в базу

- сделан тестовый запрос `SELECT * FROM churn_data LIMIT 2;`

## Используемый стек

**Библиотеки Python**

- pandas — обработка данных

- matplotlib/seaborn — визуализации

- SQLAlchemy + psycopg2 — работа с PostgreSQL

- python-dotenv — безопасное хранение конфигурации

**СУБД**

- PostgreSQL

## Подготовка окружения

```
pip install -r requirements.txt
```

Создать файл .env:

```
PGHOST=localhost
PGPORT=5432
PGDATABASE=churn_database
PGUSER=churn_user
PGPASSWORD=strong_password
```

## Итоги проекта

- Построен полный пайплайн анализа данных

- Данные очищены и расширены новыми признаками

- Изучены ключевые факторы, влияющие на отток клиентов

- Финальная таблица успешно загружена в PostgreSQL

- Построен интерактивный Дашборд